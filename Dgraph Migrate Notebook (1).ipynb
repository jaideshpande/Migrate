{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9187b326",
   "metadata": {},
   "source": [
    "## Dgraph Migrate + Live Loader tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdff13c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "latest: Pulling from dgraph/standalone\n",
      "\n",
      "\u001b[1Bdc954fb5: Pulling fs layer \n",
      "\u001b[1Bb700ef54: Pulling fs layer \n",
      "\u001b[1B3d98f3d8: Pulling fs layer \n",
      "\u001b[1B0af53e79: Pulling fs layer \n",
      "\u001b[1Bacacb54b: Pulling fs layer \n",
      "\u001b[1B5d8d161a: Pulling fs layer \n",
      "\u001b[1BDigest: sha256:840b8125bd22c4519f3cfc0ffebed6ae8b027d698f945a8ff7a0377cf89eae63\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\n",
      "Status: Downloaded newer image for dgraph/standalone:latest\n",
      "docker.io/dgraph/standalone:latest\n"
     ]
    }
   ],
   "source": [
    "!docker pull dgraph/standalone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f65b77",
   "metadata": {},
   "source": [
    "We have pulled the dgraph/standalone docker image. This next code block prints the imageID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c491aad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ac77647e1c49'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imageSearchResult = !docker images | grep dgraph/standalone | grep latest | awk '{print $3}'\n",
    "imageID = imageSearchResult[0]\n",
    "imageID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033f2e71",
   "metadata": {},
   "source": [
    "We will run the docker container in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afceda4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --bg --out /tmp/output.txt\n",
    "\n",
    "docker run dgraph/standalone -p 8080:8080"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6510acda",
   "metadata": {},
   "source": [
    "The docker container ID will be used in every step of this tutorial. We save our docker container ID as a variable containerID for easy reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7088ca62",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yl/t6vrf0x924101dwl1_2r2v6w0000gn/T/ipykernel_15893/147789711.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcontainerSearchResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetoutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"docker ps | grep dgraph/standalone | awk '{print $1}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcontainerID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontainerSearchResult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcontainerID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "containerSearchResult = !docker ps | grep dgraph/standalone | awk '{print $1}'\n",
    "containerID = containerSearchResult[0]\n",
    "containerID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e068af",
   "metadata": {},
   "source": [
    "Before installing MySQL inside the docker container, we must update the apt package manager. Call Apt-get update from within the docker container as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4380bde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:2 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1091 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3420 kB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2925 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1391 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [32.0 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2789 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2619 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [29.3 kB]\n",
      "Fetched 27.8 MB in 4s (7900 kB/s)                           \n",
      "Reading package lists... Done\n"
     ]
    }
   ],
   "source": [
    "!docker exec -it $containerID apt-get update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a68659a",
   "metadata": {},
   "source": [
    "We set the front end to be noninteractive to preserve the Jupyter notebook format. Use apt-get to install the mysql-server inside our docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15eabec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "mysql-server is already the newest version (8.0.34-0ubuntu0.20.04.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!docker exec -it $containerID bash -c \"DEBIAN_FRONTEND=noninteractive apt-get -y install mysql-server\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25239a6c",
   "metadata": {},
   "source": [
    "Run mysqld in the background. Output prints mysql instance running so it is clear for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7966f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mysql     1058  2.1  4.9 2381512 396168 pts/0  Sl   17:56   0:02 mysqld\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -it $containerID bash -c 'mysqld &'\n",
    "!docker exec -it $containerID bash -c 'ps auwx | grep mysqld | grep -v grep'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3073146",
   "metadata": {},
   "source": [
    "Now we can make mysql commands from within the docker container. Before adding data, we need to create a database. Here our example database is called testdb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d1e0d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec -it $containerID mysql -u root -e 'create database testdb'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c477b6b",
   "metadata": {},
   "source": [
    "Set a password for the mysql databse with mysql_native_password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f838bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec -it $containerID mysql -u root -e \"alter user root@localhost identified with mysql_native_password by 'password';\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e171ce",
   "metadata": {},
   "source": [
    "Make a file with our login credentials. For our example we set password=password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e56a855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec -it $containerID rm -f /tmp/login.cnf \n",
    "!docker exec -it $containerID bash -c \"echo '[client]' >> /tmp/login.cnf\" \n",
    "!docker exec -it $containerID bash -c \"echo 'password=password' >> /tmp/login.cnf\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f7af1a",
   "metadata": {},
   "source": [
    "Now we can create a table in the mysql database. For this example we've created a table called Person which includes two columns, Name and Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1a4be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec -it $containerID mysql --defaults-extra-file=/tmp/login.cnf testdb -e 'create table Roster (Name varchar(50), Jersey int);'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7839a8",
   "metadata": {},
   "source": [
    "Add a few rows of data into the database. This data will eventually be converted into N-quad entries for loading into Dgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba24a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec -it $containerID mysql --defaults-extra-file=/tmp/login.cnf testdb -e 'insert into Roster values (\"LeBron\",23);'\n",
    "!docker exec -it $containerID mysql --defaults-extra-file=/tmp/login.cnf testdb -e 'insert into Roster values (\"Steph\",30);'\n",
    "!docker exec -it $containerID mysql --defaults-extra-file=/tmp/login.cnf testdb -e 'insert into Roster values (\"Kobe\",24);'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "12e000cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -it $containerID bash -c \"cd /tmp ; pwd\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e9dde7",
   "metadata": {},
   "source": [
    "Now that we have a running mysql database running inside the docker container, we can use the migrate tool to generate our schema and output files which will then be loaded into Dgraph. For more details on dgraph migrate, visit this link: https://dgraph.io/docs/migration/migrate-tool/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "47e4a85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping table Roster\r\n",
      "Dumping table constraints Roster\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -it $containerID bash -c \"cd /tmp ; rm -f schema.txt sql.rdf ; dgraph migrate --db testdb --user root --password password\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7301a92",
   "metadata": {},
   "source": [
    "Change the schema.txt file so that we have indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0e1e2e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roster.Jersey: int .\r\n",
      "Roster.Name: string @index(exact) @index(exact) .\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -it $containerID bash -c \"cd /tmp; cat schema.txt | sed 's/int/int/' | sed 's/string/string @index(exact)/' > tmp.txt ; mv tmp.txt schema.txt\"\n",
    "!docker exec -it $containerID cat /tmp/schema.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a1fbff",
   "metadata": {},
   "source": [
    "Delete all data in Dgraph so that we don't get duplicates when we re-run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c7c973d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26861ab36825\n",
      "{\"data\":{\"code\":\"Success\",\"message\":\"Done\"}}"
     ]
    }
   ],
   "source": [
    "print (containerID)\n",
    "!docker exec -it 26861ab36825 curl localhost:8080/alter -X POST -H 'Content-Type: application/dql' -d '{\"drop_all\":true}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ab0aa749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0818 19:39:41.302428    1928 init.go:85] \n",
      "\n",
      "Dgraph version   : v23.0.1\n",
      "Dgraph codename  : dgraph\n",
      "Dgraph SHA-256   : 1d9145cf378b4e97b5f6cefd55069973c65c36f07b8e05a24b68b8ff5c5e74a4\n",
      "Commit SHA-1     : 3de01e4\n",
      "Commit timestamp : 2023-07-03 14:35:26 +0530\n",
      "Branch           : HEAD\n",
      "Go version       : go1.19.10\n",
      "jemalloc enabled : true\n",
      "\n",
      "For Dgraph official documentation, visit https://dgraph.io/docs.\n",
      "For discussions about Dgraph     , visit https://discuss.dgraph.io.\n",
      "For fully-managed Dgraph Cloud   , visit https://dgraph.io/cloud.\n",
      "\n",
      "Licensed variously under the Apache Public License 2.0 and Dgraph Community License.\n",
      "Copyright 2015-2023 Dgraph Labs, Inc.\n",
      "\n",
      "\n",
      "\n",
      "Running transaction with dgraph endpoint: 127.0.0.1:9080\n",
      "\n",
      "Processing schema file \"schema.txt\"\n",
      "Processed schema file \"schema.txt\"\n",
      "\n",
      "Found 1 data file(s) to process\n",
      "Processing data file \"sql.rdf\"\n",
      "Number of TXs run            : 1                                                                    \n",
      "Number of N-Quads processed  : 18\n",
      "Time spent                   : 448.087884ms\n",
      "N-Quads processed per second : 18\n"
     ]
    }
   ],
   "source": [
    "!docker exec -it $containerID bash -c \"cd /tmp ; dgraph live --files sql.rdf --schema schema.txt --format=rdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c1113e",
   "metadata": {},
   "source": [
    "Now run some queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fc6b21eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   361  100   313  100    48  34777   5333 --:--:-- --:--:-- --:--:-- 40111\n",
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"data\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "    \u001b[0m\u001b[34;1m\"boo\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\n",
      "      \u001b[1;39m{\n",
      "        \u001b[0m\u001b[34;1m\"Roster.Name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Steph\"\u001b[0m\u001b[1;39m\n",
      "      \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "      \u001b[1;39m{\n",
      "        \u001b[0m\u001b[34;1m\"Roster.Name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Steph\"\u001b[0m\u001b[1;39m\n",
      "      \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "      \u001b[1;39m{\n",
      "        \u001b[0m\u001b[34;1m\"Roster.Name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Steph\"\u001b[0m\u001b[1;39m\n",
      "      \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
      "    \u001b[1;39m]\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"extensions\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "    \u001b[0m\u001b[34;1m\"server_latency\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "      \u001b[0m\u001b[34;1m\"parsing_ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m90039\u001b[0m\u001b[1;39m,\n",
      "      \u001b[0m\u001b[34;1m\"processing_ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1095934\u001b[0m\u001b[1;39m,\n",
      "      \u001b[0m\u001b[34;1m\"encoding_ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1196024\u001b[0m\u001b[1;39m,\n",
      "      \u001b[0m\u001b[34;1m\"assign_timestamp_ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1273607\u001b[0m\u001b[1;39m,\n",
      "      \u001b[0m\u001b[34;1m\"total_ns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m3830530\u001b[0m\u001b[1;39m\n",
      "    \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"txn\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "      \u001b[0m\u001b[34;1m\"start_ts\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m147\u001b[0m\u001b[1;39m\n",
      "    \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0m\u001b[34;1m\"metrics\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "      \u001b[0m\u001b[34;1m\"num_uids\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
      "        \u001b[0m\u001b[34;1m\"Roster.Name\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m3\u001b[0m\u001b[1;39m,\n",
      "        \u001b[0m\u001b[34;1m\"_total\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m3\u001b[0m\u001b[1;39m\n",
      "      \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
      "    \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!docker exec -it 26861ab36825 bash -c \"curl localhost:8080/query -X POST -H 'Content-Type: application/dql' -d '{boo(func: eq(Roster.Name, \"Steph\")){Roster.Name}}' | jq\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ddcf0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
